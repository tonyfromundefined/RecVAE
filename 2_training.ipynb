{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### movielens-20m dataset이 있는 곳\n",
    "DATA_DIR = './dataset/small/'\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, 'output')\n",
    "# OUTPUT_MODEL_DIR = os.path.join(DATA_DIR, 'models/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_train_data`\n",
    "\n",
    "csv 파일을 불러와서, Sparse matrix로 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file, n_items, n_users, global_indexing=False):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    \n",
    "    n_users = n_users if global_indexing else tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "\n",
    "    data = sparse.csr_matrix(\n",
    "        (np.ones_like(rows), (rows, cols)),\n",
    "        dtype='float64',\n",
    "        shape=(n_users, n_items)\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_tr_te_data`\n",
    "\n",
    "csv 파일을 불러와서 uid의 offset을 맞춰준 뒤 Sparse matrix로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te, n_items, n_users, global_indexing=False):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    if global_indexing:\n",
    "        start_idx = 0\n",
    "        end_idx = len(unique_uid) - 1\n",
    "    else:\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix(\n",
    "        (np.ones_like(rows_tr), (rows_tr, cols_tr)),\n",
    "        dtype='float64',\n",
    "        shape=(end_idx - start_idx + 1, n_items)\n",
    "    )\n",
    "    data_te = sparse.csr_matrix(\n",
    "        (np.ones_like(rows_te), (rows_te, cols_te)),\n",
    "        dtype='float64',\n",
    "        shape=(end_idx - start_idx + 1, n_items)\n",
    "    )\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_data`\n",
    "\n",
    "전체 데이터를 가져옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(global_indexing=False):\n",
    "    UNIQUE_SID_TXT_PATH = os.path.join(OUTPUT_DIR, 'unique_sid.txt')\n",
    "    UNIQUE_UID_TXT_PATH = os.path.join(OUTPUT_DIR, 'unique_uid.txt')\n",
    "    TRAIN_CSV_PATH = os.path.join(OUTPUT_DIR, 'train.csv')\n",
    "    VALIDATION_TR_CSV_PATH = os.path.join(OUTPUT_DIR, 'validation_tr.csv')\n",
    "    VALIDATION_TE_CSV_PATH = os.path.join(OUTPUT_DIR, 'validation_te.csv')\n",
    "    TEST_TR_CSV_PATH = os.path.join(OUTPUT_DIR, 'test_tr.csv')\n",
    "    TEST_TE_CSV_PATH = os.path.join(OUTPUT_DIR, 'test_te.csv')\n",
    "\n",
    "    unique_sid = list()\n",
    "    unique_uid = list()\n",
    "\n",
    "    with open(UNIQUE_SID_TXT_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            unique_sid.append(line.strip())\n",
    "\n",
    "    with open(UNIQUE_UID_TXT_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            unique_uid.append(line.strip())\n",
    "\n",
    "    n_items = len(unique_sid)\n",
    "    n_users = len(unique_uid)\n",
    "\n",
    "    train_data = load_train_data(\n",
    "        TRAIN_CSV_PATH,\n",
    "        n_items,\n",
    "        n_users,\n",
    "        global_indexing=global_indexing\n",
    "    )\n",
    "    vad_data_tr, vad_data_te = load_tr_te_data(\n",
    "        VALIDATION_TR_CSV_PATH,\n",
    "        VALIDATION_TE_CSV_PATH,\n",
    "        n_items,\n",
    "        n_users,\n",
    "        global_indexing=global_indexing\n",
    "    )\n",
    "    test_data_tr, test_data_te = load_tr_te_data(\n",
    "        TEST_TR_CSV_PATH,\n",
    "        TEST_TE_CSV_PATH,\n",
    "        n_items,\n",
    "        n_users, \n",
    "        global_indexing=global_indexing\n",
    "    )\n",
    "\n",
    "    return train_data, vad_data_tr, vad_data_te, test_data_tr, test_data_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NDCG_binary_at_k_batch`\n",
    "\n",
    "모르겠음. 뭔가 정확도 검증하는 함수일거같은데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Recall_at_k_batch`\n",
    "\n",
    "모르겠음. 뭔가 정확도 검증하는 함수일거같은데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서부터 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "seed = 92819\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (m.astype('float32') for m in get_data(global_indexing=False))\n",
    "\n",
    "train_data, valid_1_data, valid_2_data, test_1_data, test_2_data = data\n",
    "n_users, n_items = train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ipykernel\n",
    "import requests\n",
    "from requests.compat import urljoin\n",
    "from notebook.notebookapp import list_running_servers\n",
    "\n",
    "def get_notebook_name():\n",
    "    kernel_id = re.search('kernel-(.*).json', ipykernel.connect.get_connection_file()).group(1)\n",
    "    servers = list_running_servers()\n",
    "    for ss in servers:\n",
    "        response = requests.get(urljoin(ss['url'], 'api/sessions'), params={'token': ss.get('token', '')})\n",
    "        for nn in json.loads(response.text):\n",
    "            if nn['kernel']['id'] == kernel_id:\n",
    "                relative_path = nn['path']\n",
    "                return relative_path.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_model_fn = 'model_' + get_notebook_name().replace(' ', '_') + '.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish Activation Function\n",
    "\n",
    "https://data-newbie.tistory.com/262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def swish_(x):\n",
    "    return x.mul_(torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "\n",
    "- entrophy: https://hyunw.kim/blog/2017/10/14/Entropy.html\n",
    "- cross-entrophy: https://hyunw.kim/blog/2017/10/26/Cross_Entropy.html\n",
    "- KL-divergence: https://hyunw.kim/blog/2017/10/27/KL_divergence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(q_distr, p_distr, weights, eps=1e-7):\n",
    "    mu_q, logvar_q = q_distr\n",
    "    mu_p, logvar_p = p_distr\n",
    "    return 0.5 * (((logvar_q.exp() + (mu_q - mu_p).pow(2)) / (logvar_p.exp() + eps) + logvar_p - logvar_q - 1).sum(dim=-1) * weights).mean()\n",
    "\n",
    "def simple_kl(mu_q, logvar_q, logvar_p_scale, norm):\n",
    "    return (-0.5 * ((1 + logvar_q - mu_q.pow(2)/logvar_p_scale - logvar_q.exp()/logvar_p_scale)).sum(dim=-1) * norm).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `log_norm_pdf`\n",
    "\n",
    "log-normal probability density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5 * (logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "def log_norm_std_pdf(x):\n",
    "    return -0.5 * (np.log(2 * np.pi) + x.pow(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DeterministicDecoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicDecoder(nn.Linear):\n",
    "    def __init__(self, *args):\n",
    "        super(DeterministicDecoder, self).__init__(*args)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        output = super(DeterministicDecoder, self).forward(*args)\n",
    "        return output, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StochasticDecoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDecoder(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(StochasticDecoder, self).__init__(in_features, out_features, bias)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.logvar = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.logvar.data.fill_(-2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            std = torch.exp(self.logvar)\n",
    "            a = F.linear(input, self.weight, self.bias)\n",
    "            eps = torch.randn_like(a)\n",
    "            b = eps.mul_(torch.sqrt_(F.linear(input * input, std)))\n",
    "            output = a + b\n",
    "\n",
    "            kl = simple_kl(self.weight, self.logvar, 1, 1)\n",
    "            # kl = (-0.5 * (1 + self.logvar - self.weight.pow(2) - self.logvar.exp())).sum(dim=-1).mean()\n",
    "            return output, kl\n",
    "\n",
    "        else:\n",
    "            output = F.linear(input, self.weight, self.bias)\n",
    "            return output, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GaussianMixturePrior`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixturePrior(nn.Module):\n",
    "    def __init__(self, latent_dim, gaussians_number):\n",
    "        super(GaussianMixturePrior, self).__init__()\n",
    "        \n",
    "        self.gaussians_number = gaussians_number\n",
    "        \n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(latent_dim, gaussians_number))\n",
    "        self.mu_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(latent_dim, gaussians_number))\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        density_per_gaussian = log_norm_pdf(\n",
    "            x=z[:, :, None],\n",
    "            mu=self.mu_prior[None, ...].detach(),\n",
    "            logvar=self.logvar_prior[None, ...].detach()\n",
    "        ).add(-np.log(self.gaussians_number))\n",
    "        \n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GaussianMixturePriorWithAprPost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixturePriorWithAprPost(nn.Module):\n",
    "    def __init__(self, latent_dim, input_count):\n",
    "        super(GaussianMixturePriorWithAprPost, self).__init__()\n",
    "        \n",
    "        self.gaussians_number = 1\n",
    "\n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(latent_dim, self.gaussians_number))\n",
    "        self.mu_prior.data.fill_(0)\n",
    "\n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(latent_dim, self.gaussians_number))\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "\n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(\n",
    "            latent_dim,\n",
    "            self.gaussians_number\n",
    "        ))\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "\n",
    "        self.user_mu = nn.Embedding(input_count, latent_dim)\n",
    "        self.user_logvar = nn.Embedding(input_count, latent_dim)\n",
    "\n",
    "    def forward(self, z, idx):\n",
    "        density_per_gaussian1 = log_norm_pdf(\n",
    "            x=z[:, :, None],\n",
    "            mu=self.mu_prior[None, :, :].detach(),\n",
    "            logvar=self.logvar_prior[None, :, :].detach()\n",
    "        ).add(np.log(1/5 - 1/20))\n",
    "\n",
    "        density_per_gaussian2 = log_norm_pdf(\n",
    "            x=z[:, :, None],\n",
    "            mu=self.user_mu(idx)[:, :, None].detach(),\n",
    "            logvar=self.user_logvar(idx)[:, :, None].detach()\n",
    "        ).add(np.log(4/5 - 1/20))\n",
    "\n",
    "        density_per_gaussian3 = log_norm_pdf(\n",
    "            x=z[:, :, None],\n",
    "            mu=self.mu_prior[None, :, :].detach(),\n",
    "            logvar=self.logvar_uniform_prior[None, :, :].detach()\n",
    "        ).add(np.log(1/10))\n",
    "\n",
    "        density_per_gaussian = torch.cat([\n",
    "            density_per_gaussian1,\n",
    "            density_per_gaussian2,\n",
    "            density_per_gaussian3\n",
    "        ], dim=-1)\n",
    "\n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VAE`\n",
    "\n",
    "모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, matrix_dim, axis):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(matrix_dim[1], hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.prior = GaussianMixturePriorWithAprPost(latent_dim, matrix_dim[0])\n",
    "        self.decoder = DeterministicDecoder(latent_dim, matrix_dim[1])        \n",
    "        self.axis = axis\n",
    "\n",
    "\n",
    "    def encode(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "    \n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "        \n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "\n",
    "        return self.fc21(h5), self.fc22(h5)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        user_ratings,\n",
    "        user_idx,\n",
    "        beta=1,\n",
    "        dropout_rate=0.5,\n",
    "        calculate_loss=True,\n",
    "        mode=None\n",
    "    ):\n",
    "        if mode == 'pr':\n",
    "            mu, logvar = self.encode(user_ratings, dropout_rate=dropout_rate)\n",
    "        elif mode == 'mf':\n",
    "            mu, logvar = self.encode(user_ratings, dropout_rate=0)\n",
    "            \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        x_pred, decoder_loss = self.decode(z)\n",
    "        \n",
    "        NLL = -(F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "        \n",
    "        if calculate_loss:\n",
    "            if mode == 'pr':\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                KLD = -(self.prior(z, user_idx) - log_norm_pdf(z, mu, logvar)).sum(dim=-1).mul(norm).mean()\n",
    "                loss = NLL + beta * KLD + decoder_loss\n",
    "\n",
    "            elif mode == 'mf':\n",
    "                KLD = NLL * 0\n",
    "                loss = NLL + decoder_loss\n",
    "            \n",
    "            return (NLL, KLD), loss\n",
    "            \n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def set_embeddings(self, train_data, momentum=0, weight=None):\n",
    "        istraining = self.training\n",
    "        self.eval()\n",
    "\n",
    "        for batch in generate(\n",
    "            batch_size=500,\n",
    "            device=device,\n",
    "            data_1=train_data,\n",
    "            axis=self.axis\n",
    "        ):\n",
    "            user_ratings = batch.get_ratings_to_dev()\n",
    "            users_idx = batch.get_idx()\n",
    "\n",
    "            new_user_mu, new_user_logvar = self.encode(user_ratings, 0)\n",
    "\n",
    "            old_user_mu = self.prior.user_mu.weight.data[users_idx,:].detach()\n",
    "            old_user_logvar = self.prior.user_logvar.weight.data[users_idx,:].detach()\n",
    "\n",
    "            if weight:\n",
    "                old_user_var = torch.exp(old_user_logvar)\n",
    "                new_user_var = torch.exp(new_user_logvar)\n",
    "\n",
    "                post_user_var = 1 / (1 / old_user_var + weight / new_user_var)\n",
    "                post_user_mu = (old_user_mu / old_user_var + weight * new_user_mu / new_user_var) * post_user_var\n",
    "\n",
    "                self.prior.user_mu.weight.data[users_idx,:] = post_user_mu\n",
    "                self.prior.user_logvar.weight.data[users_idx,:] = torch.log(post_user_var + new_user_var)\n",
    "            else:\n",
    "                self.prior.user_mu.weight.data[users_idx,:] = momentum * old_user_mu + (1-momentum) * new_user_mu\n",
    "                self.prior.user_logvar.weight.data[users_idx,:] = momentum * old_user_logvar + (1-momentum) * new_user_logvar\n",
    "\n",
    "        if istraining:\n",
    "            self.train()\n",
    "        else:\n",
    "            self.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `generate`\n",
    "\n",
    "`Batch` 함수를 generator로 이용할 수 있도록 wrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    batch_size,\n",
    "    device,\n",
    "    axis,\n",
    "    data_1,\n",
    "    data_2=None,\n",
    "    shuffle=False,\n",
    "    samples_perc_per_epoch=1\n",
    "):\n",
    "    assert axis in ['users', 'items']\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "    \n",
    "    if axis == 'items':\n",
    "        data_1 = data_1.T\n",
    "        if data_2 is not None:\n",
    "            data_2 = data_2.T\n",
    "    \n",
    "    total_samples = data_1.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "    \n",
    "    \n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "    \n",
    "    for st_idx in tqdm(range(0, samples_per_epoch, batch_size)):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_1, data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Batch`\n",
    "\n",
    "데이터를 쪼개서 각각의 환경으로 나눔."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, device, idx, data_1, data_2=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_1 = data_1\n",
    "        self._data_2 = data_2\n",
    "    \n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "    \n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "\n",
    "    def get_ratings(self, is_test=False):\n",
    "        data = self._data_2 if is_test else self._data_1\n",
    "        return data[self._idx]\n",
    "    \n",
    "    def get_ratings_to_dev(self, is_test=False):\n",
    "        return torch.Tensor(self.get_ratings(is_test).toarray()).to(self._device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `validate`\n",
    "\n",
    "NDCG? 로 검증하는 코드인듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_1, data_2, axis, mode, samples_perc_per_epoch=1):\n",
    "    model.eval()\n",
    "    batch_size = 500\n",
    "    ndcg_dist = []\n",
    "    \n",
    "    \n",
    "    for batch in generate(\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        axis=axis,\n",
    "        data_1=data_1,\n",
    "        data_2=data_2,\n",
    "        samples_perc_per_epoch=samples_perc_per_epoch\n",
    "    ):\n",
    "        ratings = batch.get_ratings_to_dev()\n",
    "        idx = batch.get_idx_to_dev()\n",
    "        ratings_test = batch.get_ratings(is_test=True)\n",
    "    \n",
    "        pred_val = model.forward(ratings, idx, calculate_loss=False, mode=mode).cpu().detach().numpy()\n",
    "        \n",
    "        if not (data_1 is data_2):\n",
    "            pred_val[batch.get_ratings().nonzero()] = -np.inf\n",
    "\n",
    "        ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, ratings_test))\n",
    "\n",
    "    ndcg_dist = np.concatenate(ndcg_dist)\n",
    "\n",
    "    return ndcg_dist[~np.isnan(ndcg_dist)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, opts, train_data, batch_size, n_epochs, axis, beta, mode):\n",
    "    global best_ndcg\n",
    "    global ndcgs_tr_pr, ndcgs_tr_mf, ndcgs_va_pr, ndcgs_va_mf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        NLL_loss = 0\n",
    "        KLD_loss = 0\n",
    "\n",
    "        for batch in generate(\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            axis=axis,\n",
    "            data_1=train_data,\n",
    "            shuffle=True\n",
    "        ):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "            idx = batch.get_idx_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "            (NLL, KLD), loss = model.forward(ratings, idx, beta=beta, mode=mode)\n",
    "            loss.backward()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.step()\n",
    "\n",
    "            NLL_loss += NLL.item()\n",
    "            KLD_loss += KLD.item()\n",
    "\n",
    "\n",
    "        print('NLL_loss', NLL_loss, 'KLD_loss', KLD_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_opts`\n",
    "\n",
    "Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opts(model, lr=5e-4):\n",
    "    decoder_params = set(model.decoder.parameters())\n",
    "    embedding_params = set(model.prior.user_mu.parameters()) | set(model.prior.user_logvar.parameters())\n",
    "    encoder_params = set(model.parameters()) - decoder_params - embedding_params\n",
    "\n",
    "    optimizer_encoder = optim.Adam(encoder_params, lr=lr)\n",
    "    optimizer_decoder = optim.Adam(decoder_params, lr=lr)\n",
    "    optimizer_embedding = optim.Adam(embedding_params, lr=lr)\n",
    "\n",
    "    print('encoder\\n', [x.shape for x in encoder_params])\n",
    "    print('decoder\\n', [x.shape for x in decoder_params])\n",
    "    print('embedding\\n', [x.shape for x in embedding_params])\n",
    "    \n",
    "    return optimizer_encoder, optimizer_decoder, optimizer_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ndcg = -np.inf\n",
    "ndcgs_tr_pr, ndcgs_tr_mf, ndcgs_va_pr, ndcgs_va_mf = [], [], [], []\n",
    "var_param_distance = []\n",
    "\n",
    "hidden_dim = 600\n",
    "latent_dim = 200\n",
    "\n",
    "model_i = VAE(hidden_dim, latent_dim, (n_users, n_items), 'users').to(device)\n",
    "model_i.set_embeddings(train_data)\n",
    "\n",
    "optimizer_encoder_i, optimizer_decoder_i, _ = get_opts(model_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "        model_i,\n",
    "        [optimizer_encoder_i],\n",
    "        train_data,\n",
    "        batch_size=500,\n",
    "        n_epochs=3,\n",
    "        axis='users',\n",
    "        mode='pr',\n",
    "        beta=0.005\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    run(\n",
    "        model_i,\n",
    "        [optimizer_encoder_i],\n",
    "        train_data,\n",
    "        batch_size=500,\n",
    "        n_epochs=3,\n",
    "        axis='users',\n",
    "        mode='pr',\n",
    "        beta=0.005\n",
    "    )\n",
    "\n",
    "    model_i.set_embeddings(train_data)\n",
    "    run(\n",
    "        model_i,\n",
    "        [optimizer_decoder_i],\n",
    "        train_data,\n",
    "        batch_size=500,\n",
    "        n_epochs=1,\n",
    "        axis='users',\n",
    "        mode='mf',\n",
    "        beta=None\n",
    "    )\n",
    "    \n",
    "    model = model_i\n",
    "    axis = 'users'\n",
    "    \n",
    "    ndcg_ = validate(model, train_data, train_data, axis, 'mf', 0.01)\n",
    "    ndcgs_tr_mf.append(ndcg_)\n",
    "\n",
    "    ndcg_ = validate(model, train_data, train_data, axis, 'pr', 0.01)\n",
    "    ndcgs_tr_pr.append(ndcg_)\n",
    "    \n",
    "    ndcg_ = validate(model, valid_1_data, valid_2_data, axis, 'pr', 1)\n",
    "    ndcgs_va_pr.append(ndcg_)\n",
    "\n",
    "    \n",
    "    clear_output(True)\n",
    "    \n",
    "    i_min = np.array(ndcgs_va_pr).argsort()[-len(ndcgs_va_pr)//2:].min()\n",
    "\n",
    "    print('ndcg', ndcgs_va_pr[-1], ': : :', best_ndcg)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(15,5)\n",
    "\n",
    "    ax1.plot(range(i_min, len(ndcgs_va_pr)), ndcgs_va_pr[i_min:], '+-', label='pr valid')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(i_min, len(ndcgs_va_pr)), ndcgs_tr_pr[i_min:], '+:', label='pr train')\n",
    "    ax2.plot(range(i_min, len(ndcgs_va_pr)), ndcgs_tr_mf[i_min:], 'x:', label='mf train')\n",
    "    ax2.legend(loc='lower left')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.ylabel(\"Validation NDCG@100\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    if ndcg_ > best_ndcg:\n",
    "        best_ndcg = ndcg_\n",
    "        torch.save(model.state_dict(), ser_model_fn)\n",
    "        \n",
    "    if ndcg_ < best_ndcg / 2 and epoch > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitvenvvirtualenvec8cb82ba2ad4e4cb51e3df6fa660abe",
   "display_name": "Python 3.7.6 64-bit ('.venv': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}